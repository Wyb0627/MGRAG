"""
LMCacheæ¨¡å‹é…ç½®æ–‡ä»¶
ç”¨äºæ ¹æ®å®é™…ä½¿ç”¨çš„æ¨¡å‹è®¾ç½®æ­£ç¡®çš„LMCacheå‚æ•°
"""

import torch
from typing import Dict, Any, Optional


class ModelConfig:
    """æ¨¡å‹é…ç½®åŸºç±»"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    def get_lmcache_metadata(self) -> Dict[str, Any]:
        """è·å–LMCacheå…ƒæ•°æ®"""
        raise NotImplementedError
    
    def get_gpu_connector_params(self) -> Dict[str, Any]:
        """è·å–GPUè¿æ¥å™¨å‚æ•°"""
        raise NotImplementedError
    
    def get_auto_hyperparams(self, gpu_memory_gb: float = 24.0) -> Dict[str, Any]:
        """
        æ ¹æ®æ¨¡å‹ç‰¹æ€§å’ŒGPUå†…å­˜è‡ªåŠ¨è®¾ç½®è¶…å‚æ•°
        
        Args:
            gpu_memory_gb: GPUå†…å­˜å¤§å°ï¼ˆGBï¼‰
            
        Returns:
            è‡ªåŠ¨é…ç½®çš„è¶…å‚æ•°å­—å…¸
        """
        raise NotImplementedError
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹åŸºæœ¬ä¿¡æ¯"""
        raise NotImplementedError


class Qwen25VL7BConfig(ModelConfig):
    """Qwen2.5-VL-7Bæ¨¡å‹é…ç½®"""
    
    def __init__(self):
        super().__init__("Qwen/Qwen2.5-VL-7B-Instruct")
    
    def get_lmcache_metadata(self) -> Dict[str, Any]:
        return {
            "model_name": "qwen2.5-vl-7b",
            "world_size": 1,
            "worker_id": 0,
            "fmt": "vllm",
            "kv_dtype": torch.bfloat16,
            "kv_shape": (32, 2, 256, 32, 128),  # (num_layers, 2, chunk_size, num_kv_head, head_size)
            "use_mla": False
        }
    
    def get_gpu_connector_params(self) -> Dict[str, Any]:
        return {
            "hidden_dim_size": 4096,  # 32 * 128 = 4096
            "num_layers": 32,
            "chunk_size": 256,
            "kv_dtype": torch.bfloat16,
            "device": "cuda",
            "use_mla": False
        }
    
    def get_auto_hyperparams(self, gpu_memory_gb: float = 24.0) -> Dict[str, Any]:
        """
        ä¸ºQwen2.5-VL-7Bæ¨¡å‹è‡ªåŠ¨è®¾ç½®è¶…å‚æ•°
        
        åŸºäºæ¨¡å‹ç‰¹æ€§ï¼š
        - 7Bå‚æ•°æ¨¡å‹ï¼Œç›¸å¯¹è¾ƒå°
        - 32å±‚ï¼Œæ¯å±‚4096ç»´
        - é€‚åˆä¸­ç­‰GPUå†…å­˜
        """
        
        # åŸºç¡€é…ç½®
        base_config = {
            'chunk_size': 256,
            'enable_blending': True,
            'blend_mode': 'graphrag',
            'blend_separator': '[LMCACHE_BLEND_SEP]',
            'blend_add_special_in_precomp': False,
            'enable_p2p': False,
            'pipelined_backend': False,
            'save_decode_cache': False,
            'remote_serde': 'torch'
        }
        
        # æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´
        if gpu_memory_gb >= 40:
            # å¤§å†…å­˜GPUï¼šå¯ä»¥è®¾ç½®æ›´å¤§çš„ç¼“å­˜
            base_config.update({
                'max_local_cache_size': 20.0,
                'blend_recompute_ratio': 0.10,  # æ›´å°‘çš„é‡æ–°è®¡ç®—
                'blend_min_tokens': 128,         # æ›´çŸ­çš„åºåˆ—ä¹Ÿèƒ½æ··åˆ
                'local_device': 'cuda'
            })
        elif gpu_memory_gb >= 24:
            # ä¸­ç­‰å†…å­˜GPUï¼šå¹³è¡¡é…ç½®
            base_config.update({
                'max_local_cache_size': 12.0,
                'blend_recompute_ratio': 0.15,
                'blend_min_tokens': 256,
                'local_device': 'cuda'
            })
        elif gpu_memory_gb >= 16:
            # å°å†…å­˜GPUï¼šä¿å®ˆé…ç½®
            base_config.update({
                'max_local_cache_size': 8.0,
                'blend_recompute_ratio': 0.20,  # æ›´å¤šé‡æ–°è®¡ç®—ä»¥èŠ‚çœå†…å­˜
                'blend_min_tokens': 512,        # æ›´é•¿çš„åºåˆ—æ‰æ··åˆ
                'local_device': 'cuda'
            })
        else:
            # æå°å†…å­˜GPUï¼šä½¿ç”¨CPUç¼“å­˜
            base_config.update({
                'max_local_cache_size': 5.0,
                'blend_recompute_ratio': 0.25,
                'blend_min_tokens': 1024,
                'local_device': 'cpu'
            })
        
        return base_config
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–Qwen2.5-VL-7Bæ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': 'Qwen/Qwen2.5-VL-7B-Instruct',
            'model_size': '7B',
            'architecture': 'Qwen2.5-VL',
            'num_layers': 32,
            'num_kv_head': 32,
            'head_size': 128,
            'hidden_dim': 4096,
            'vocab_size': 151936,
            'max_seq_len': 32768,
            'multimodal': True,
            'vision_tower': 'Qwen2.5-VL',
            'recommended_gpu_memory': '16GB+',
            'optimal_chunk_size': 256,
            'optimal_blend_ratio': 0.15
        }


class Qwen25VL32BConfig(ModelConfig):
    """Qwen2.5-VL-32Bæ¨¡å‹é…ç½®"""
    
    def __init__(self):
        super().__init__("Qwen/Qwen2.5-VL-32B-Instruct")
    
    def get_lmcache_metadata(self) -> Dict[str, Any]:
        return {
            "model_name": "qwen2.5-vl-32b",
            "world_size": 1,
            "worker_id": 0,
            "fmt": "vllm",
            "kv_dtype": torch.bfloat16,
            "kv_shape": (64, 2, 256, 32, 128),  # (num_layers, 2, chunk_size, num_kv_head, head_size)
            "use_mla": False
        }
    
    def get_gpu_connector_params(self) -> Dict[str, Any]:
        return {
            "hidden_dim_size": 4096,  # 32 * 128 = 4096
            "num_layers": 64,
            "chunk_size": 256,
            "kv_dtype": torch.bfloat16,
            "device": "cuda",
            "use_mla": False
        }
    
    def get_auto_hyperparams(self, gpu_memory_gb: float = 24.0) -> Dict[str, Any]:
        """
        ä¸ºQwen2.5-VL-32Bæ¨¡å‹è‡ªåŠ¨è®¾ç½®è¶…å‚æ•°
        
        åŸºäºæ¨¡å‹ç‰¹æ€§ï¼š
        - 32Bå‚æ•°æ¨¡å‹ï¼Œè¾ƒå¤§
        - 64å±‚ï¼Œæ¯å±‚4096ç»´
        - éœ€è¦å¤§GPUå†…å­˜
        """
        
        # åŸºç¡€é…ç½®
        base_config = {
            'chunk_size': 256,
            'enable_blending': True,
            'blend_mode': 'graphrag',
            'blend_separator': '[LMCACHE_BLEND_SEP]',
            'blend_add_special_in_precomp': False,
            'enable_p2p': False,
            'pipelined_backend': False,
            'save_decode_cache': False,
            'remote_serde': 'torch'
        }
        
        # æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´ï¼ˆ32Bæ¨¡å‹éœ€è¦æ›´å¤šå†…å­˜ï¼‰
        if gpu_memory_gb >= 80:
            # è¶…å¤§å†…å­˜GPU
            base_config.update({
                'max_local_cache_size': 30.0,
                'blend_recompute_ratio': 0.08,
                'blend_min_tokens': 128,
                'local_device': 'cuda'
            })
        elif gpu_memory_gb >= 48:
            # å¤§å†…å­˜GPU
            base_config.update({
                'max_local_cache_size': 20.0,
                'blend_recompute_ratio': 0.12,
                'blend_min_tokens': 256,
                'local_device': 'cuda'
            })
        elif gpu_memory_gb >= 32:
            # ä¸­ç­‰å†…å­˜GPUï¼šä¿å®ˆé…ç½®
            base_config.update({
                'max_local_cache_size': 15.0,
                'blend_recompute_ratio': 0.18,
                'blend_min_tokens': 512,
                'local_device': 'cuda'
            })
        else:
            # å°å†…å­˜GPUï¼šä½¿ç”¨CPUç¼“å­˜
            base_config.update({
                'max_local_cache_size': 8.0,
                'blend_recompute_ratio': 0.25,
                'blend_min_tokens': 1024,
                'local_device': 'cpu'
            })
        
        return base_config
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–Qwen2.5-VL-32Bæ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': 'Qwen/Qwen2.5-VL-32B-Instruct',
            'model_size': '32B',
            'architecture': 'Qwen2.5-VL',
            'num_layers': 64,
            'num_kv_head': 32,
            'head_size': 128,
            'hidden_dim': 4096,
            'vocab_size': 151936,
            'max_seq_len': 32768,
            'multimodal': True,
            'vision_tower': 'Qwen2.5-VL',
            'recommended_gpu_memory': '48GB+',
            'optimal_chunk_size': 256,
            'optimal_blend_ratio': 0.12
        }


class MiMoVL7BConfig(ModelConfig):
    """MiMo-VL-7Bæ¨¡å‹é…ç½®"""
    
    def __init__(self):
        super().__init__("XiaomiMiMo/MiMo-VL-7B-RL")
    
    def get_lmcache_metadata(self) -> Dict[str, Any]:
        return {
            "model_name": "mimo-vl-7b",
            "world_size": 1,
            "worker_id": 0,
            "fmt": "vllm",
            "kv_dtype": torch.float16,
            "kv_shape": (32, 2, 256, 32, 128),  # (num_layers, 2, chunk_size, num_kv_head, head_size)
            "use_mla": False
        }
    
    def get_gpu_connector_params(self) -> Dict[str, Any]:
        return {
            "hidden_dim_size": 4096,  # 32 * 128 = 4096
            "num_layers": 32,
            "chunk_size": 256,
            "kv_dtype": torch.float16,
            "device": "cuda",
            "use_mla": False
        }


class LlavaNext8BConfig(ModelConfig):
    """LLaVA-NeXT-8Bæ¨¡å‹é…ç½®"""
    
    def __init__(self):
        super().__init__("llava-hf/llama3-llava-next-8b-hf")
    
    def get_lmcache_metadata(self) -> Dict[str, Any]:
        return {
            "model_name": "llava-next-8b",
            "world_size": 1,
            "worker_id": 0,
            "fmt": "vllm",
            "kv_dtype": torch.float16,
            "kv_shape": (32, 2, 256, 32, 128),  # (num_layers, 2, chunk_size, num_kv_head, head_size)
            "use_mla": False
        }
    
    def get_gpu_connector_params(self) -> Dict[str, Any]:
        return {
            "hidden_dim_size": 4096,  # 32 * 128 = 4096
            "num_layers": 32,
            "chunk_size": 256,
            "kv_dtype": torch.float16,
            "device": "cuda",
            "use_mla": False
        }


def get_model_config(model_name: str) -> Optional[ModelConfig]:
    """
    æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        
    Returns:
        å¯¹åº”çš„æ¨¡å‹é…ç½®å¯¹è±¡ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    
    # æ¨¡å‹åç§°æ˜ å°„
    model_configs = {
        "Qwen/Qwen2.5-VL-7B-Instruct": Qwen25VL7BConfig(),
        "Qwen/Qwen2.5-VL-32B-Instruct": Qwen25VL32BConfig(),
        "XiaomiMiMo/MiMo-VL-7B-RL": MiMoVL7BConfig(),
        "llava-hf/llama3-llava-next-8b-hf": LlavaNext8BConfig(),
        "qwen": Qwen25VL7BConfig(),  # é»˜è®¤ä½¿ç”¨7Bç‰ˆæœ¬
        "mimo": MiMoVL7BConfig(),
        "llava": LlavaNext8BConfig(),
    }
    
    # ç²¾ç¡®åŒ¹é…
    if model_name in model_configs:
        return model_configs[model_name]
    
    # æ¨¡ç³ŠåŒ¹é…
    for key, config in model_configs.items():
        if model_name.lower() in key.lower() or key.lower() in model_name.lower():
            return config
    
    return None


def create_custom_model_config(
    model_name: str,
    num_layers: int,
    num_kv_head: int,
    head_size: int,
    kv_dtype: torch.dtype = torch.bfloat16,
    chunk_size: int = 256
) -> ModelConfig:
    """
    åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°
        num_layers: å±‚æ•°
        num_kv_head: KVå¤´æ•°
        head_size: å¤´å¤§å°
        kv_dtype: KVæ•°æ®ç±»å‹
        chunk_size: å—å¤§å°
        
    Returns:
        è‡ªå®šä¹‰æ¨¡å‹é…ç½®å¯¹è±¡
    """
    
    class CustomModelConfig(ModelConfig):
        def __init__(self, model_name, num_layers, num_kv_head, head_size, kv_dtype, chunk_size):
            super().__init__(model_name)
            self.num_layers = num_layers
            self.num_kv_head = num_kv_head
            self.head_size = head_size
            self.kv_dtype = kv_dtype
            self.chunk_size = chunk_size
        
        def get_lmcache_metadata(self) -> Dict[str, Any]:
            return {
                "model_name": model_name.lower().replace("/", "-").replace("_", "-"),
                "world_size": 1,
                "worker_id": 0,
                "fmt": "vllm",
                "kv_dtype": self.kv_dtype,
                "kv_shape": (self.num_layers, 2, self.chunk_size, self.num_kv_head, self.head_size),
                "use_mla": False
            }
        
        def get_gpu_connector_params(self) -> Dict[str, Any]:
            return {
                "hidden_dim_size": self.num_kv_head * self.head_size,
                "num_layers": self.num_layers,
                "chunk_size": self.chunk_size,
                "kv_dtype": self.kv_dtype,
                "device": "cuda",
                "use_mla": False
            }
    
    return CustomModelConfig(model_name, num_layers, num_kv_head, head_size, kv_dtype, chunk_size)


def print_model_config_info(model_config: ModelConfig):
    """
    æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
    
    Args:
        model_config: æ¨¡å‹é…ç½®å¯¹è±¡
    """
    
    print(f"Model: {model_config.model_name}")
    print("LMCache Metadata:")
    metadata = model_config.get_lmcache_metadata()
    for key, value in metadata.items():
        print(f"  {key}: {value}")
    
    print("GPU Connector Parameters:")
    gpu_params = model_config.get_gpu_connector_params()
    for key, value in gpu_params.items():
        print(f"  {key}: {value}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è·å–é¢„å®šä¹‰æ¨¡å‹é…ç½®
    qwen_config = get_model_config("Qwen/Qwen2.5-VL-7B-Instruct")
    if qwen_config:
        print_model_config_info(qwen_config)
    
    # åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹é…ç½®
    custom_config = create_custom_model_config(
        "custom-model",
        num_layers=48,
        num_kv_head=64,
        head_size=128,
        kv_dtype=torch.float16,
        chunk_size=512
    )
    print("\nCustom Model Config:")
    print_model_config_info(custom_config)


def detect_gpu_memory() -> float:
    """
    æ£€æµ‹GPUå†…å­˜å¤§å°
    
    Returns:
        GPUå†…å­˜å¤§å°ï¼ˆGBï¼‰ï¼Œå¦‚æœæ£€æµ‹å¤±è´¥è¿”å›é»˜è®¤å€¼24.0
    """
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_memory_gb = gpu_memory / (1024**3)
            print(f"Detected GPU memory: {gpu_memory_gb:.1f} GB")
            return gpu_memory_gb
        else:
            print("CUDA not available, using default GPU memory: 24.0 GB")
            return 24.0
    except Exception as e:
        print(f"Failed to detect GPU memory: {e}, using default: 24.0 GB")
        return 24.0


def get_optimal_config_for_model(
    model_name: str, 
    gpu_memory_gb: Optional[float] = None,
    auto_detect_gpu: bool = True
) -> Dict[str, Any]:
    """
    ä¸ºæŒ‡å®šæ¨¡å‹è·å–æœ€ä¼˜é…ç½®
    
    Args:
        model_name: æ¨¡å‹åç§°
        gpu_memory_gb: GPUå†…å­˜å¤§å°ï¼ˆGBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
        auto_detect_gpu: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹GPUå†…å­˜
        
    Returns:
        åŒ…å«æ‰€æœ‰é…ç½®çš„å­—å…¸
    """
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = get_model_config(model_name)
    if model_config is None:
        print(f"Warning: No predefined config found for model {model_name}")
        return {}
    
    # æ£€æµ‹GPUå†…å­˜
    if gpu_memory_gb is None and auto_detect_gpu:
        gpu_memory_gb = detect_gpu_memory()
    elif gpu_memory_gb is None:
        gpu_memory_gb = 24.0  # é»˜è®¤å€¼
    
    # è·å–è‡ªåŠ¨è¶…å‚æ•°
    hyperparams = model_config.get_auto_hyperparams(gpu_memory_gb)
    
    # è·å–GPUè¿æ¥å™¨å‚æ•°
    gpu_params = model_config.get_gpu_connector_params()
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = model_config.get_model_info()
    
    # åˆå¹¶æ‰€æœ‰é…ç½®
    config = {
        'model_info': model_info,
        'gpu_connector_params': gpu_params,
        'hyperparams': hyperparams,
        'detected_gpu_memory_gb': gpu_memory_gb
    }
    
    return config


def print_optimal_config(config: Dict[str, Any]):
    """
    æ‰“å°æœ€ä¼˜é…ç½®ä¿¡æ¯
    
    Args:
        config: é…ç½®å­—å…¸
    """
    
    if not config:
        print("No configuration available")
        return
    
    print("=" * 60)
    print("LMCache Optimal Configuration")
    print("=" * 60)
    
    # æ¨¡å‹ä¿¡æ¯
    if 'model_info' in config:
        print("\nğŸ“‹ Model Information:")
        model_info = config['model_info']
        for key, value in model_info.items():
            print(f"  {key}: {value}")
    
    # GPUè¿æ¥å™¨å‚æ•°
    if 'gpu_connector_params' in config:
        print("\nğŸ”§ GPU Connector Parameters:")
        gpu_params = config['gpu_connector_params']
        for key, value in gpu_params.items():
            print(f"  {key}: {value}")
    
    # è¶…å‚æ•°
    if 'hyperparams' in config:
        print("\nâš™ï¸  Auto-configured Hyperparameters:")
        hyperparams = config['hyperparams']
        for key, value in hyperparams.items():
            print(f"  {key}: {value}")
    
    # GPUå†…å­˜ä¿¡æ¯
    if 'detected_gpu_memory_gb' in config:
        print(f"\nğŸ’¾ Detected GPU Memory: {config['detected_gpu_memory_gb']:.1f} GB")
    
    print("=" * 60)


def create_lmcache_config_file(
    model_name: str,
    output_path: str = "lmcache_optimal_config.yaml",
    gpu_memory_gb: Optional[float] = None
):
    """
    åˆ›å»ºLMCacheæœ€ä¼˜é…ç½®æ–‡ä»¶
    
    Args:
        model_name: æ¨¡å‹åç§°
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        gpu_memory_gb: GPUå†…å­˜å¤§å°ï¼ˆGBï¼‰
    """
    
    try:
        import yaml
        
        # è·å–æœ€ä¼˜é…ç½®
        config = get_optimal_config_for_model(model_name, gpu_memory_gb)
        
        if not config:
            print("Failed to get configuration")
            return
        
        # å‡†å¤‡YAMLé…ç½®
        yaml_config = {
            'model': config.get('model_info', {}),
            'gpu_connector': config.get('gpu_connector_params', {}),
            'hyperparameters': config.get('hyperparams', {}),
            'gpu_memory_gb': config.get('detected_gpu_memory_gb', 24.0)
        }
        
        # å†™å…¥YAMLæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_config, f, default_flow_style=False, indent=2, allow_unicode=True)
        
        print(f"Configuration saved to: {output_path}")
        
    except ImportError:
        print("PyYAML not available, cannot create config file")
    except Exception as e:
        print(f"Failed to create config file: {e}")


# ä¾¿æ·å‡½æ•°
def auto_configure_qwen_7b(gpu_memory_gb: Optional[float] = None) -> Dict[str, Any]:
    """ä¸ºQwen2.5-VL-7Bæ¨¡å‹è‡ªåŠ¨é…ç½®"""
    return get_optimal_config_for_model("Qwen/Qwen2.5-VL-7B-Instruct", gpu_memory_gb)


def auto_configure_qwen_32b(gpu_memory_gb: Optional[float] = None) -> Dict[str, Any]:
    """ä¸ºQwen2.5-VL-32Bæ¨¡å‹è‡ªåŠ¨é…ç½®"""
    return get_optimal_config_for_model("Qwen/Qwen2.5-VL-32B-Instruct", gpu_memory_gb)


def auto_configure_mimo_7b(gpu_memory_gb: Optional[float] = None) -> Dict[str, Any]:
    """ä¸ºMiMo-VL-7Bæ¨¡å‹è‡ªåŠ¨é…ç½®"""
    return get_optimal_config_for_model("XiaomiMiMo/MiMo-VL-7B-RL", gpu_memory_gb)
